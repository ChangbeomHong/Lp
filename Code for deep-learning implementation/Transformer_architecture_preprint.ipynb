{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5089671f-c742-4a37-ba80-b746944b8c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6252be15-d094-4c3c-9f33-ec7cacfd796d",
   "metadata": {},
   "source": [
    "# Load data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae73de5-10e9-4b8b-894e-1de3f1e8c36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_x = np.load('MinMax_scaled_x.npy') # MinMax-scaled x coordinates\n",
    "tmp_y = np.load('MinMax_scaled_y.npy') # MinMax-scaled x coordinates\n",
    "y = np.load('log_Lp.npy') # Target value (Lp)\n",
    "Lw = tmp_x.shape[1] # Window size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57813ba9-2c14-498b-ae0e-c4b7bbf3c7f6",
   "metadata": {},
   "source": [
    "# Construct training & validation data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52f62a0-be08-4a8d-a47b-9efda302c31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((tmp_x.shape[0],tmp_x.shape[1],2)) # Input data\n",
    "for i in range(x.shape[0]) :\n",
    "    x[i,:,0] = tmp_x[i]\n",
    "    x[i,:,1] = tmp_y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bb6802-282a-47b8-a10f-1cd033d113e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data set into training and validation data sets\n",
    "n_samples = x.shape[0]\n",
    "IDX = np.arange(0,n_samples,1,dtype = 'int')\n",
    "np.random.shuffle(IDX)\n",
    "train_x = x[IDX[:int(n_samples*0.9)]]\n",
    "test_x = x[IDX[int(n_samples*0.9):]]\n",
    "train_y = y[IDX[:int(n_samples*0.9)]]\n",
    "test_y = y[IDX[int(n_samples*0.9):]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2437e495-69be-4649-bb0c-a6b8928bc64f",
   "metadata": {},
   "source": [
    "# Implement transformer-based architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64888346-ca83-4efe-8c06-6f28de295989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE\n",
    "This code provides the architectural skeleton of our Transformer-based model used in the manuscript.\n",
    "The detailed implementation is currently withheld and will be released upon acceptance.\n",
    "\n",
    "- `TransformerBlock`: Multi-head attention and feedforward layers (details hidden)\n",
    "- `PositionalEncoding`: Trainable positional embedding (details hidden)\n",
    "- `build_transformer_model`: Builds the full Keras model (hidden)\n",
    "\n",
    "For review purposes, a minimal subset of code may be shared upon request.\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        raise NotImplementedError(\"TransformerBlock implementation is withheld during the preprint stage.\")\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        raise NotImplementedError(\"TransformerBlock forward pass is withheld during the preprint stage.\")\n",
    "\n",
    "\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, sequence_length, embed_dim):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.embed_dim = embed_dim\n",
    "        raise NotImplementedError(\"PositionalEncoding implementation is withheld during the preprint stage.\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        raise NotImplementedError(\"PositionalEncoding call method is withheld during the preprint stage.\")\n",
    "\n",
    "\n",
    "def build_transformer_model(sequence_length, embed_dim=64, num_heads=64, ff_dim=128, num_layers=5, dropout_rate=0):\n",
    "    \"\"\"\n",
    "    Build a Transformer-based regression model for sequence input of shape (batch_size, sequence_length, 2).\n",
    "    Actual implementation of TransformerBlock and PositionalEncoding is omitted.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Model construction is withheld during the preprint stage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9843659-2483-4b6e-ad07-75fab2b908f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = build_transformer_model(Lw)\n",
    "X.compile(optimizer = Adam(10**-4), loss = 'MAE')\n",
    "X.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f3d6d9-7c55-4b3d-8483-2be54982e5f8",
   "metadata": {},
   "source": [
    "# Training setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031627d4-eb76-45fd-90ff-09fe18ad9e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_FOLDER_PATH = './'\n",
    "model_path = MODEL_SAVE_FOLDER_PATH + 'Model.hdf5'\n",
    "cb_checkpoint = ModelCheckpoint(filepath=model_path, monitor='val_loss', mode = 'min', verbose=1, save_best_only=True) # Model checkpoint\n",
    "cb_early_stopping = EarlyStopping(monitor='val_loss', mode = 'min', patience=50, restore_best_weights=True) # Early stopping\n",
    "lr_reduction = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=10**-0.5, patience=15, min_lr=1e-7) # Reduce learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8780b802-c580-4a87-89c8-be67836c9d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = X.fit(train_x,train_y, epochs=10000,batch_size = 2**10, callbacks=[cb_checkpoint,cb_early_stopping,SaveHistory(),lr_reduction],validation_data=(test_x,test_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
