{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5089671f-c742-4a37-ba80-b746944b8c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6252be15-d094-4c3c-9f33-ec7cacfd796d",
   "metadata": {},
   "source": [
    "# Load data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae73de5-10e9-4b8b-894e-1de3f1e8c36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_x = np.load('MinMax_scaled_x.npy') # MinMax-scaled x coordinates\n",
    "tmp_y = np.load('MinMax_scaled_y.npy') # MinMax-scaled x coordinates\n",
    "y = np.load('log_Lp.npy') # Target value (Lp)\n",
    "Lw = tmp_x.shape[1] # Window size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57813ba9-2c14-498b-ae0e-c4b7bbf3c7f6",
   "metadata": {},
   "source": [
    "# Construct training & validation data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52f62a0-be08-4a8d-a47b-9efda302c31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((tmp_x.shape[0],tmp_x.shape[1],2)) # Input data\n",
    "for i in range(x.shape[0]) :\n",
    "    x[i,:,0] = tmp_x[i]\n",
    "    x[i,:,1] = tmp_y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bb6802-282a-47b8-a10f-1cd033d113e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data set into training and validation data sets\n",
    "n_samples = x.shape[0]\n",
    "IDX = np.arange(0,n_samples,1,dtype = 'int')\n",
    "np.random.shuffle(IDX)\n",
    "train_x = x[IDX[:int(n_samples*0.9)]]\n",
    "test_x = x[IDX[int(n_samples*0.9):]]\n",
    "train_y = y[IDX[:int(n_samples*0.9)]]\n",
    "test_y = y[IDX[int(n_samples*0.9):]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2437e495-69be-4649-bb0c-a6b8928bc64f",
   "metadata": {},
   "source": [
    "# Implement transformer-based architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64888346-ca83-4efe-8c06-6f28de295989",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation=\"relu\"),\n",
    "            layers.Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, sequence_length, embed_dim):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.position_embeddings = self.add_weight(\n",
    "            shape=(1, sequence_length, embed_dim), \n",
    "            initializer=\"zeros\", \n",
    "            trainable=True, \n",
    "            name=\"pos_embedding\"\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.position_embeddings\n",
    "\n",
    "def build_transformer_model(sequence_length, embed_dim=64, num_heads=64, ff_dim=128, num_layers=5, dropout_rate=0):\n",
    "    inputs = keras.Input(shape=(sequence_length, 2))  # (batch_size, sequence_length, 2)\n",
    "    x = layers.Dense(embed_dim)(inputs)  # Linear projection to embedding size\n",
    "    x = PositionalEncoding(sequence_length, embed_dim)(x)\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        x = TransformerBlock(embed_dim, num_heads, ff_dim, dropout_rate)(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D()(x)  # Pooling over sequence dimension\n",
    "    outputs = layers.Dense(1, activation = 'relu')(x)  # Scalar output\n",
    "\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9843659-2483-4b6e-ad07-75fab2b908f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = build_transformer_model(Lw)\n",
    "X.compile(optimizer = Adam(10**-4), loss = 'MAE')\n",
    "X.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f3d6d9-7c55-4b3d-8483-2be54982e5f8",
   "metadata": {},
   "source": [
    "# Training setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031627d4-eb76-45fd-90ff-09fe18ad9e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVE_FOLDER_PATH = './'\n",
    "model_path = MODEL_SAVE_FOLDER_PATH + 'Model.hdf5'\n",
    "cb_checkpoint = ModelCheckpoint(filepath=model_path, monitor='val_loss', mode = 'min', verbose=1, save_best_only=True) # Model checkpoint\n",
    "cb_early_stopping = EarlyStopping(monitor='val_loss', mode = 'min', patience=50, restore_best_weights=True) # Early stopping\n",
    "lr_reduction = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=10**-0.5, patience=15, min_lr=1e-7) # Reduce learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8780b802-c580-4a87-89c8-be67836c9d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = X.fit(train_x,train_y, epochs=10000,batch_size = 2**10, callbacks=[cb_checkpoint,cb_early_stopping,SaveHistory(),lr_reduction],validation_data=(test_x,test_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
